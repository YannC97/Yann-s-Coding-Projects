{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1xWr/7IfC88tw+2xhQVgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YannC97/Yann-s-Coding-Projects/blob/Codes/Asset%20Pricing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Suppose there is a single agent with β = 0.95,\n",
        "v(c) = log(c), there are two states\n",
        "which are iid with equal probabilities π1 = π2 = 0.5 and the firm’s production\n",
        "function is\n",
        "f(k, 1) = 0.9k\n",
        "0.3 + 0.9k, f(k, 2) = 1.1k\n",
        "0.3 + 0.9k\n",
        "\n",
        "Discretize the possible capital values to 50 points. Use value function iteration\n",
        "to compute the policy functions (one for each shock) for consumption and investment.\n",
        "Plot these functions. Now use 500 points for admissible capital levels\n",
        "and redo the exercise.\n",
        "\n",
        "b)\n",
        "- Redo the same exercise with β = 0.99 and with v(c) = −c^−4\n",
        "- Also redo the exercise with β = 0.95, log-utility but with persistent shocks.\n",
        "Assume that π(1, 1) = π(2, 2) = 0.95.\n",
        "\n",
        "For each case, compute the standard deviations of investment, consumption and output.\n",
        "Also compute the average return to capital and the average risk-free rate.\n",
        "Note that there is no bond in the setup of the problem, but with one\n",
        "agent, adding a bond does not change anything and hence you can read off the\n",
        "risk-free rate from your solutions using the Euler equation.'''"
      ],
      "metadata": {
        "id": "Y_UmKZL2pEC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-ydPLHKMin6n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import root\n",
        "from scipy.interpolate import CubicSpline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "alpha = 0.3 # Capital share in output\n",
        "beta = 0.95 # Discount factor #later on Beta =\n",
        "delta = 0.1 # Depreciation rate\n",
        "A_low, A_high = 0.9, 1.1 # Productivity states\n",
        "pi = np.array([0.5, 0.5]) # Transition probabilities (i.i.d.)\n",
        "k_min, k_max, num_k = 0.1, 10, 50 # Capital grid\n",
        "k_grid = np.linspace(k_min, k_max, num_k)\n",
        "max_iter = 500\n",
        "tol = 1e-6"
      ],
      "metadata": {
        "id": "PHW6B-k1pL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7b6q8fGmpMS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfZjzYUvpMYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Production function f(k, A)\n",
        "def f(k, A):\n",
        "return A * k**alpha + (1 - delta) * k\n",
        "\n",
        "# Utility function u(c) = -c^(-4)\n",
        "def u(c):\n",
        "#return -(np.maximum(c, 1e-8)) ** (-4) # Avoid singularity at c = 0\n",
        "return np.log(c)\n",
        "\n",
        "# Consumption function ensuring positivity\n",
        "def c(k, A, k_prime):\n",
        "return np.maximum(f(k, A) - k_prime, 1e-8) # Ensure consumption is positive\n",
        "\n",
        "# Marginal utility function\n",
        "def mu_c(c):\n",
        "return 4 * c**(-5)\n",
        "\n",
        "# FOC for root-finding\n",
        "def belman_foc(k_prime, k, A, V_prime, z):\n",
        "return -mu_c(c(k, A, k_prime)) + beta * V_prime[z](k_prime)\n",
        "\n",
        "# Howard's Policy Iteration\n",
        "def policy_iteration(max_iter=500, tol=1e-6):\n",
        "policy = np.full((num_k, 2), 0.5 * k_grid[:, None]) # Initial policy guess\n",
        "V = np.zeros((num_k, 2)) # Initial value function\n",
        "for iteration in range(max_iter):\n",
        "\n",
        "# Step 1: Policy Evaluation\n",
        "V_new = np.zeros_like(V)\n",
        "V_splines = [CubicSpline(k_grid, V[:, z], bc_type='natural') for z in range(2)]\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "k_next = policy[i, z]\n",
        "V_new[i, z] = u(c(k, A, k_next)) + beta * np.dot(pi, [V_splines[0](k_next), V_splines[1](k_next)])\n",
        "\n",
        "# Check convergence\n",
        "V_converged = np.max(np.abs(V_new - V)) < tol\n",
        "V = V_new # Update value function\n",
        "\n",
        "# Step 2: Policy Improvement\n",
        "policy_stable = True\n",
        "V_prime_splines = [CubicSpline(k_grid, np.gradient(V[:, z], k_grid), bc_type='natural') for z in range(2)]\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "solution = root(belman_foc, x0=policy[i, z], args=(k, A, V_prime_splines, z))\n",
        "k_opt = solution.x[0] if solution.success and solution.x[0] > 0 else max(1e-6, f(k, A) * 0.5)\n",
        "if np.abs(k_opt - policy[i, z]) > tol:\n",
        "policy_stable = False\n",
        "policy[i, z] = k_opt\n",
        "if policy_stable and V_converged:\n",
        "break\n",
        "return V, policy\n",
        "\n",
        "# Compute solution\n",
        "V_policy, policy_policy = policy_iteration()\n",
        "#3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "#https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 1/8\n",
        "#Analytical Steady-State Capital: 4.1870\n",
        "#Numerical Steady-State Capital (Low A): 2.1402\n",
        "#Numerical Steady-State Capital (High A): 6.8648\n",
        "\n",
        "# Plot Value Function\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_grid, V_policy[:, 0], 'b--', label='Value Function (Low A)')\n",
        "plt.plot(k_grid, V_policy[:, 1], 'orange', label='Value Function (High A)')\n",
        "plt.xlabel('Capital')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.title('Value Function')\n",
        "\n",
        "# Plot Policy Function\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_grid, policy_policy[:, 0], 'r--', label='Policy Function (Low A)')\n",
        "plt.plot(k_grid, policy_policy[:, 1], 'b', label='Policy Function (High A)')\n",
        "plt.xlabel('Capital Today')\n",
        "plt.ylabel('Capital Tomorrow')\n",
        "plt.legend()\n",
        "plt.title('Policy Function')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare steady-state values\n",
        "k_star_numerical_low = policy_policy[np.argmin(np.abs(policy_policy[:, 0] - k_grid)), 0]\n",
        "k_star_numerical_high = policy_policy[np.argmin(np.abs(policy_policy[:, 1] - k_grid)), 1]\n",
        "k_star_analytical = ((1 - beta * (1 - delta)) / (beta * alpha)) ** (1 / (alpha - 1))\n",
        "print(f\"Analytical Steady-State Capital: {k_star_analytical:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (Low A): {k_star_numerical_low:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (High A): {k_star_numerical_high:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TUqIovxcpMcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ctVDDwQrrswj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import root\n",
        "from scipy.interpolate import CubicSpline\n",
        "# Parameters\n",
        "alpha = 0.3 # Capital share in output\n",
        "beta = 0.95 # Discount factor\n",
        "delta = 0.1 # Depreciation rate\n",
        "A_low, A_high = 0.9, 1.1 # Productivity states\n",
        "pi = np.array([0.5, 0.5]) # Transition probabilities (i.i.d.)\n",
        "k_min, k_max, num_k = 0.1, 10, 200 # Capital grid\n",
        "k_grid = np.linspace(k_min, k_max, num_k)\n",
        "max_iter = 500\n",
        "tol = 1e-6\n",
        "3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 2/8\n",
        "# Production function f(k, A)\n",
        "def f(k, A):\n",
        "return A * k**alpha + (1 - delta) * k\n",
        "# Utility function u(c) = log(c)\n",
        "def u(c):\n",
        "return np.log(np.maximum(c, 1e-8)) # Avoid log(0) issues\n",
        "# Consumption function ensuring positivity\n",
        "def c(k, A, k_prime):\n",
        "return np.maximum(f(k, A) - k_prime, 1e-8) # Ensure consumption is positive\n",
        "# Marginal utility function u'(c) = 1/c\n",
        "def mu_c(c):\n",
        "return 1 / np.maximum(c, 1e-8) # Avoid division by zero\n",
        "# First-order condition for root-finding\n",
        "def bellman_foc(k_prime, k, A, V_prime, z):\n",
        "return -mu_c(c(k, A, k_prime)) + beta * V_prime[z](k_prime)\n",
        "# Howard's Policy Iteration\n",
        "def policy_iteration(max_iter=500, tol=1e-6):\n",
        "policy = np.full((num_k, 2), 0.5 * k_grid[:, None]) # Initial policy guess\n",
        "V = np.zeros((num_k, 2)) # Initial value function\n",
        "for iteration in range(max_iter):\n",
        "# Step 1: Policy Evaluation\n",
        "V_new = np.zeros_like(V)\n",
        "V_splines = [CubicSpline(k_grid, V[:, z], bc_type='natural') for z in range(2)]\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "k_next = policy[i, z]\n",
        "V_new[i, z] = u(c(k, A, k_next)) + beta * np.dot(pi, [V_splines[0](k_next), V_splines[1](k_next)])\n",
        "# Check convergence\n",
        "V_converged = np.max(np.abs(V_new - V)) < tol\n",
        "V = V_new # Update value function\n",
        "# Step 2: Policy Improvement\n",
        "policy_stable = True\n",
        "V_prime_splines = [CubicSpline(k_grid, np.gradient(V[:, z], k_grid), bc_type='natural') for z in range(2)]\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "solution = root(bellman_foc, x0=policy[i, z], args=(k, A, V_prime_splines, z))\n",
        "k_opt = solution.x[0] if solution.success and solution.x[0] > 0 else max(1e-6, f(k, A) * 0.5)\n",
        "if np.abs(k_opt - policy[i, z]) > tol:\n",
        "policy_stable = False\n",
        "policy[i, z] = k_opt\n",
        "if policy_stable and V_converged:\n",
        "break\n",
        "return V, policy\n",
        "# Compute solution\n",
        "V_policy, policy_policy = policy_iteration()\n",
        "# Plot Value Function\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_grid, V_policy[:, 0], 'b--', label='Value Function (Low A)')\n",
        "plt.plot(k_grid, V_policy[:, 1], 'orange', label='Value Function (High A)')\n",
        "plt.xlabel('Capital')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.title('Value Function')\n",
        "# Plot Policy Function\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_grid, policy_policy[:, 0], 'r--', label='Policy Function (Low A)')\n",
        "plt.plot(k_grid, policy_policy[:, 1], 'b', label='Policy Function (High A)')\n",
        "plt.xlabel('Capital Today')\n",
        "plt.ylabel('Capital Tomorrow')\n",
        "3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 3/8\n",
        "plt.legend()\n",
        "plt.title('Policy Function')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Compare steady-state values\n",
        "k_star_numerical_low = policy_policy[np.argmin(np.abs(policy_policy[:, 0] - k_grid)), 0]\n",
        "k_star_numerical_high = policy_policy[np.argmin(np.abs(policy_policy[:, 1] - k_grid)), 1]\n",
        "k_star_analytical = ((1 - beta * (1 - delta)) / (beta * alpha)) ** (1 / (alpha - 1))\n",
        "print(f\"Analytical Steady-State Capital: {k_star_analytical:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (Low A): {k_star_numerical_low:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (High A): {k_star_numerical_high:.4f}\")\n",
        "Analytical Steady-State Capital: 2.6257\n",
        "Numerical Steady-State Capital (Low A): 2.0869\n",
        "Numerical Steady-State Capital (High A): 3.2820\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import root\n",
        "from scipy.interpolate import interp1d\n",
        "# Parameters\n",
        "alpha = 0.3 # Capital share in output\n",
        "beta = 0.99 # Discount factor\n",
        "delta = 0.1 # Depreciation rate\n",
        "A_low, A_high = 0.9, 1.1 # Productivity states\n",
        "pi = np.array([0.5, 0.5]) # Transition probabilities (i.i.d.)\n",
        "k_min, k_max, num_k = 0.1, 10, 200 # Capital grid\n",
        "k_grid = np.linspace(k_min, k_max, num_k)\n",
        "max_iter = 500\n",
        "tol = 1e-6\n",
        "# Production function f(k, A)\n",
        "def f(k, A):\n",
        "return A * k**alpha + (1 - delta) * k\n",
        "# Utility function u(c) = log(c)\n",
        "def u(c):\n",
        "return np.log(np.maximum(c, 1e-8)) # Avoid log(0) issues\n",
        "# Consumption function ensuring positivity\n",
        "def c(k, A, k_prime):\n",
        "return np.maximum(f(k, A) - k_prime, 1e-8) # Ensure consumption is positive\n",
        "# Marginal utility function u'(c) = 1/c\n",
        "def mu_c(c):\n",
        "return 1 / np.maximum(c, 1e-8) # Avoid division by zero\n",
        "3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 4/8\n",
        "# First-order condition for root-finding\n",
        "def bellman_foc(k_prime, k, A, V_prime, z):\n",
        "return -mu_c(c(k, A, k_prime)) + beta * np.interp(k_prime, k_grid, V_prime[z])\n",
        "# Howard's Policy Iteration (with Linear Interpolation)\n",
        "def policy_iteration(max_iter=500, tol=1e-6):\n",
        "policy = np.full((num_k, 2), 0.5 * k_grid[:, None]) # Initial policy guess\n",
        "V = np.zeros((num_k, 2)) # Initial value function\n",
        "for iteration in range(max_iter):\n",
        "# Step 1: Policy Evaluation\n",
        "V_new = np.zeros_like(V)\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "k_next = policy[i, z]\n",
        "V_interp = [interp1d(k_grid, V[:, j], kind='linear', fill_value=\"extrapolate\") for j in range(2)]\n",
        "V_new[i, z] = u(c(k, A, k_next)) + beta * np.dot(pi, [V_interp[0](k_next), V_interp[1](k_next)])\n",
        "# Check convergence\n",
        "V_converged = np.max(np.abs(V_new - V)) < tol\n",
        "V = V_new # Update value function\n",
        "# Step 2: Policy Improvement\n",
        "policy_stable = True\n",
        "V_prime_grid = [np.gradient(V[:, z], k_grid) for z in range(2)]\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "solution = root(bellman_foc, x0=policy[i, z], args=(k, A, V_prime_grid, z))\n",
        "k_opt = solution.x[0] if solution.success and solution.x[0] > 0 else max(1e-6, f(k, A) * 0.5)\n",
        "if np.abs(k_opt - policy[i, z]) > tol:\n",
        "policy_stable = False\n",
        "policy[i, z] = k_opt\n",
        "if policy_stable and V_converged:\n",
        "break\n",
        "return V, policy\n",
        "# Compute solution\n",
        "V_policy, policy_policy = policy_iteration()\n",
        "# Plot Value Function\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_grid, V_policy[:, 0], 'b--', label='Value Function (Low A)')\n",
        "plt.plot(k_grid, V_policy[:, 1], 'orange', label='Value Function (High A)')\n",
        "plt.xlabel('Capital')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.title('Value Function (Linear Interpolation)')\n",
        "# Plot Policy Function\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_grid, policy_policy[:, 0], 'r--', label='Policy Function (Low A)')\n",
        "plt.plot(k_grid, policy_policy[:, 1], 'b', label='Policy Function (High A)')\n",
        "plt.xlabel('Capital Today')\n",
        "plt.ylabel('Capital Tomorrow')\n",
        "plt.legend()\n",
        "plt.title('Policy Function (Linear Interpolation)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Compare steady-state values\n",
        "k_star_numerical_low = policy_policy[np.argmin(np.abs(policy_policy[:, 0] - k_grid)), 0]\n",
        "k_star_numerical_high = policy_policy[np.argmin(np.abs(policy_policy[:, 1] - k_grid)), 1]\n",
        "k_star_analytical = ((1 - beta * (1 - delta)) / (beta * alpha)) ** (1 / (alpha - 1))\n",
        "print(f\"Analytical Steady-State Capital: {k_star_analytical:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (Low A): {k_star_numerical_low:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (High A): {k_star_numerical_high:.4f}\")\n",
        "3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 5/8\n",
        "Analytical Steady-State Capital: 4.1870\n",
        "Numerical Steady-State Capital (Low A): 3.2839\n",
        "Numerical Steady-State Capital (High A): 5.2251\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import root\n",
        "from scipy.interpolate import interp1d\n",
        "# Parameters\n",
        "alpha = 0.3 # Capital share in output\n",
        "beta = 0.99 # Discount factor\n",
        "delta = 0.1 # Depreciation rate\n",
        "A_low, A_high = 0.9, 1.1 # Productivity states\n",
        "pi = np.array([0.5, 0.5]) # Transition probabilities (i.i.d.)\n",
        "k_min, k_max, num_k = 0.1, 10, 200 # Capital grid\n",
        "k_grid = np.linspace(k_min, k_max, num_k)\n",
        "max_iter = 500\n",
        "tol = 1e-6\n",
        "# Production function f(k, A)\n",
        "def f(k, A):\n",
        "return A * k**alpha + (1 - delta) * k\n",
        "# Utility function u(c) = log(c)\n",
        "def u(c):\n",
        "return np.log(np.maximum(c, 1e-8)) # Avoid log(0) issues\n",
        "# Consumption function ensuring positivity\n",
        "def c(k, A, k_prime):\n",
        "return np.maximum(f(k, A) - k_prime, 1e-8) # Ensure consumption is positive\n",
        "# Marginal utility function u'(c) = 1/c\n",
        "def mu_c(c):\n",
        "return 1 / np.maximum(c, 1e-8) # Avoid division by zero\n",
        "# First-order condition for root-finding\n",
        "def bellman_foc(k_prime, k, A, V_prime, z):\n",
        "return -mu_c(c(k, A, k_prime)) + beta * np.interp(k_prime, k_grid, V_prime[z])\n",
        "# Howard's Policy Iteration (with Linear Interpolation)\n",
        "def policy_iteration(max_iter=500, tol=1e-6):\n",
        "policy = np.full((num_k, 2), 0.5 * k_grid[:, None]) # Initial policy guess\n",
        "V = np.zeros((num_k, 2)) # Initial value function\n",
        "for iteration in range(max_iter):\n",
        "# Step 1: Policy Evaluation\n",
        "V_new = np.zeros_like(V)\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 6/8\n",
        "k_next = policy[i, z]\n",
        "V_interp = [interp1d(k_grid, V[:, j], kind='linear', fill_value=\"extrapolate\") for j in range(2)]\n",
        "V_new[i, z] = u(c(k, A, k_next)) + beta * np.dot(pi, [V_interp[0](k_next), V_interp[1](k_next)])\n",
        "# Check convergence\n",
        "V_converged = np.max(np.abs(V_new - V)) < tol\n",
        "V = V_new # Update value function\n",
        "# Step 2: Policy Improvement\n",
        "policy_stable = True\n",
        "V_prime_grid = [np.gradient(V[:, z], k_grid) for z in range(2)]\n",
        "for i, k in enumerate(k_grid):\n",
        "for z, A in enumerate([A_low, A_high]):\n",
        "solution = root(bellman_foc, x0=policy[i, z], args=(k, A, V_prime_grid, z))\n",
        "k_opt = solution.x[0] if solution.success and solution.x[0] > 0 else max(1e-6, f(k, A) * 0.5)\n",
        "if np.abs(k_opt - policy[i, z]) > tol:\n",
        "policy_stable = False\n",
        "policy[i, z] = k_opt\n",
        "if policy_stable and V_converged:\n",
        "break\n",
        "return V, policy\n",
        "# Compute solution\n",
        "V_policy, policy_policy = policy_iteration()\n",
        "# Plot Value Function\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_grid, V_policy[:, 0], 'b--', label='Value Function (Low A)')\n",
        "plt.plot(k_grid, V_policy[:, 1], 'orange', label='Value Function (High A)')\n",
        "plt.xlabel('Capital')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.title('Value Function (Linear Interpolation)')\n",
        "# Plot Policy Function\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_grid, policy_policy[:, 0], 'r--', label='Policy Function (Low A)')\n",
        "plt.plot(k_grid, policy_policy[:, 1], 'b', label='Policy Function (High A)')\n",
        "plt.xlabel('Capital Today')\n",
        "plt.ylabel('Capital Tomorrow')\n",
        "plt.legend()\n",
        "plt.title('Policy Function (Linear Interpolation)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Compare steady-state values\n",
        "k_star_numerical_low = policy_policy[np.argmin(np.abs(policy_policy[:, 0] - k_grid)), 0]\n",
        "k_star_numerical_high = policy_policy[np.argmin(np.abs(policy_policy[:, 1] - k_grid)), 1]\n",
        "k_star_analytical = ((1 - beta * (1 - delta)) / (beta * alpha)) ** (1 / (alpha - 1))\n",
        "print(f\"Analytical Steady-State Capital: {k_star_analytical:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (Low A): {k_star_numerical_low:.4f}\")\n",
        "print(f\"Numerical Steady-State Capital (High A): {k_star_numerical_high:.4f}\")\n",
        "3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 7/8\n",
        "Analytical Steady-State Capital: 4.1870\n",
        "Numerical Steady-State Capital (Low A): 3.2839\n",
        "Numerical Steady-State Capital (High A): 5.2251\n",
        "3/6/25, 9:49 AM Stochastic_Ramsey_Linear_vs_Cubic.ipynb - Colab\n",
        "https://colab.research.google.com/drive/1yeB1kpezETqLK3mTYKFSF7LJCFKokLl6 8/8"
      ],
      "metadata": {
        "id": "tBe7jxzrrs4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}